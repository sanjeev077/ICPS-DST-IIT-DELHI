{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "main.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z7o9TNPsfmQc"
      },
      "source": [
        "# !pip install tensorflow==2.2.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yBTIwMfgfsDT"
      },
      "source": [
        "import tensorflow\n",
        "print(tensorflow.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5kgxUSfgCC-"
      },
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19jU0byZgi9V"
      },
      "source": [
        "# !pip install pyLDAvis"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsJ3GWbQgpWo"
      },
      "source": [
        "# pip install fastText\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dyjGwqKSfd8D",
        "outputId": "3cd9cf0a-c342-432a-ef91-5c3a9ff90b27",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 140
        }
      },
      "source": [
        "# 1\n",
        "\n",
        "# importing and loading all the required libraraies and dependencies\n",
        "\n",
        "import tensorflow\n",
        "print(tensorflow.__version__)\n",
        "import tensorflow as tf\n",
        "from tensorflow.python.framework.ops import disable_eager_execution\n",
        "disable_eager_execution()\n",
        "import os\n",
        "from tensorflow.python.keras.layers import Layer\n",
        "from tensorflow.python.keras import backend as K\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "from bs4 import BeautifulSoup\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input,LSTM,Embedding,Dense,Concatenate,TimeDistributed,Bidirectional\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "import warnings \n",
        "pd.set_option(\"display.max_colwidth\",200)\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import spacy\n",
        "from spacy.lang.en import English\n",
        "\n",
        "from gensim.models.fasttext import FastText\n",
        "from string import punctuation\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk import WordPunctTokenizer\n",
        "import gensim\n",
        "from gensim.utils import simple_preprocess\n",
        "import nltk\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words=stopwords.words('english')\n",
        "stop_words.append(['from', 'subject', 're', 'edu', 'use'])\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')\n",
        "# en_stop = set(nltk.corpus.stopwords.words('english'))\n",
        "from gensim.models import CoherenceModel\n",
        "import gensim.corpora as corpora\n",
        "from pprint import pprint\n",
        "\n",
        "import pyLDAvis.gensim\n",
        "import pickle \n",
        "import pyLDAvis\n",
        "\n",
        "import pickle\n",
        "import collections\n",
        "import numpy as np\n",
        "import random\n",
        "import json\n",
        "import requests\n",
        "from tensorflow.keras.models import Model, load_model\n",
        "from tensorflow.keras.layers import Input, Embedding, dot, Reshape, Activation, Dense\n",
        "from tensorflow.keras.regularizers import l1\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from os.path import isfile\n",
        "import csv\n",
        "import heapq\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from matplotlib import pyplot \n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from tensorflow.compat.v1.keras.backend import set_session, clear_session\n",
        "from tensorflow.keras.callbacks import LambdaCallback\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import math\n",
        "import fasttext\n",
        "import copy\n",
        "from sklearn.decomposition import PCA\n",
        "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
        "nltk.download('vader_lexicon')\n",
        "import spacy\n",
        "from spacy.lang.en import English"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.3.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fsgy0WVEpVjC"
      },
      "source": [
        "# if spacy model is not available than download it from :\n",
        "# pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZDV5d2GHfd8L",
        "outputId": "e9249e26-9412-4e9c-fbea-7ad359c72377",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        }
      },
      "source": [
        "# importing all python modules where functions are defined\n",
        "\n",
        "import eval_measure\n",
        "from corpus import DocData,DocDatalstm\n",
        "import ldann\n",
        "from processing import preprocessingsum,addstartend,combinetweet\n",
        "from processing import make_bigrams,sent_to_words,lemmatization\n",
        "from ldann import Doc2Topic,Logger,data_feeder,Doc2Topiclstm\n",
        "from eval_measure import custom_evaluator\n",
        "from textsummary import AttentionLayer\n",
        "from textsummary import summarymodel,decode_sequence\n",
        "from textsummary import seq2summary,seq2text\n",
        "from fireflymod import FireflyAlgorithm\n",
        "from fireutil import bck2word,sentimentfunc,optimizeresult\n",
        "from fireutil import optimizeresultpca\n",
        "import fireutil\n",
        "from fireutil  import sent_polarity"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package vader_lexicon to\n",
            "[nltk_data]     C:\\Users\\SANJEEV\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-q0i41wpVjO"
      },
      "source": [
        "# 2\n",
        "\n",
        "\n",
        "# directory defining for input ,output,intermediate files\n",
        "input_path=\"C:/Users/SANJEEV/Desktop/datainput\"\n",
        "intermediate_path=\"C:/Users/SANJEEV/Desktop/intermedi\"\n",
        "output_path=\"E:/outputfile\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bpTC3GOMfd8P"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F2VosLEefd8S",
        "outputId": "960e7eed-781c-4e6a-8e58-812058af0403",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        }
      },
      "source": [
        "# 3\n",
        "\n",
        "# Read data into papers\n",
        "papers=pd.read_csv(os.path.join(input_path, r'Data.csv'))\n",
        "papers = papers.drop(columns=['created_at','id'\t,'id_str'\t,'truncated'\t,'entities',\t'metadata'\t,'source'\t,'in_reply_to_status_id'\t,'in_reply_to_status_id_str'\t,'in_reply_to_user_id'\t,'in_reply_to_user_id_str'\t,'in_reply_to_screen_name'\t,'user',\t'geo'\t,'coordinates'\t,'place'\t,'contributors'\t,'retweeted_status',\t'is_quote_status'\t,'retweet_count'\t,'favorite_count',\t'favorited'\t,'retweeted'\t,'lang',\t'possibly_sensitive'\t,'quoted_status_id'\t,'quoted_status_id_str'\t,'quoted_status','extended_entities'], axis=1)\n",
        "print(len(papers))\n",
        "papers.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "302005\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>0</td>\n",
              "      <td>RT @Ronald_vanLoon: Google has created a maths #AI that has already proved 1200 theorems\\n by @welcomeai |\\n\\n #ArtificialIntelligence #BigDat…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>RT @LEAD_Coalition: Curing #Alzheimers and #Parkinsons using the cloud https://t.co/7dFHplBOwq via @ToniTWhitley \\n\\n#AI #ArtificialIntellige…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>RT @Ronald_vanLoon: This Agile #Robot Can Work In Any Environment\\n by @nowthisnews |\\n #ArtificialIntelligence #AI #Robotics #IoT #InternetO…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>RT @marcusborba: Natural Language Processing used to accelerate the scientific discovery of possible new materials\\nvia @spelldotrun \\n\\n#NLP…</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>RT @Bruegel_org: Will #robots and #ArtificialIntelligence steal jobs?\\nHas technical revolution changed the economy substantially?\\nWhat will…</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                              text\n",
              "0  RT @Ronald_vanLoon: Google has created a maths #AI that has already proved 1200 theorems\\n by @welcomeai |\\n\\n #ArtificialIntelligence #BigDat…\n",
              "1   RT @LEAD_Coalition: Curing #Alzheimers and #Parkinsons using the cloud https://t.co/7dFHplBOwq via @ToniTWhitley \\n\\n#AI #ArtificialIntellige…\n",
              "2   RT @Ronald_vanLoon: This Agile #Robot Can Work In Any Environment\\n by @nowthisnews |\\n #ArtificialIntelligence #AI #Robotics #IoT #InternetO…\n",
              "3   RT @marcusborba: Natural Language Processing used to accelerate the scientific discovery of possible new materials\\nvia @spelldotrun \\n\\n#NLP…\n",
              "4   RT @Bruegel_org: Will #robots and #ArtificialIntelligence steal jobs?\\nHas technical revolution changed the economy substantially?\\nWhat will…"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfX4AEqMfd8W"
      },
      "source": [
        "# 4\n",
        "\n",
        "# data cleaning and processing\n",
        "want_train=1\n",
        "\n",
        "if want_train==1:\n",
        "    df=combinetweet(papers)\n",
        "    df.to_csv(os.path.join(intermediate_path,r'twitter_combines.csv'),index=False)\n",
        "    data_sum=pd.read_csv(os.path.join(intermediate_path,r'twitter_combines.csv'))\n",
        "    cleaned_text=data_sum.text.apply(lambda x :preprocessingsum(x))\n",
        "    cleaned_summary=data_sum.summary.apply(lambda x :preprocessingsum(x))\n",
        "    cleaned_summary.replace('',np.nan,inplace=True)\n",
        "    cleaned_summary.to_frame()\n",
        "    cleaned_summary=cleaned_summary.apply(lambda x:addstartend(x))\n",
        "    cleaned_summary.to_frame()\n",
        "    cleaned_text.to_frame()\n",
        "    cleaned_summary.to_csv(os.path.join(intermediate_path,r'cleansummary_startend_data.csv'),index=False,header=[\"summary\"])\n",
        "    cleaned_text.to_csv(os.path.join(intermediate_path,r'cleantext_startend_data.csv'),index=False,header=[\"comment\"])\n",
        "else:\n",
        "    df=combinetweet(papers)\n",
        "    df.to_csv(os.path.join(intermediate_path,r'twitter_combines.csv'),index=False)\n",
        "    data_sum=pd.read_csv(os.path.join(intermediate_path,r'twitter_combines.csv'))\n",
        "    cleaned_text=data_sum.text.apply(lambda x :preprocessingsum(x))\n",
        "    cleaned_text.to_csv(os.path.join(intermediate_path,r'cleantext_startend_data.csv'),index=False,header=[\"comment\"])\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj_KLY0MpVjl",
        "outputId": "fc33a83f-d6e3-44fa-d906-a7b06daa5eca"
      },
      "source": [
        "# 5\n",
        "\n",
        "\n",
        "papers=pd.read_csv(os.path.join(intermediate_path,r'cleantext_startend_data.csv'))    \n",
        "data = papers.values.tolist()\n",
        "data_words = list(sent_to_words(data))\n",
        "# # making Bigrams\n",
        "bigram = gensim.models.Phrases(data_words, min_count=5, threshold=100) \n",
        "bigram_mod = gensim.models.phrases.Phraser(bigram)\n",
        "data_words_bigrams = make_bigrams(data_words,bigram_mod)\n",
        "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'ner'])\n",
        "# creating lemmatized word file \n",
        "data_lemmatized = lemmatization(data_words_bigrams,nlp, allowed_postags=['NOUN', 'ADJ', 'VERB', 'ADV'])\n",
        "print(data_lemmatized[:1])\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[['curing_alzheimer', 'parkinson', 'use', 'cloud', 'agile', 'robot', 'work', 'iot', 'processing', 'use', 'accelerate', 'scientific', 'discovery', 'possible', 'new', 'material', 'robot', 'steal', 'job', 'technical', 'change', 'way', 'shape', 'future', 'learn', 'technology', 'rapid', 'parkinson', 'use', 'cloud', 'csharp', 'term', 'explain', 'context', 'blockchain', 'game', 'get', 'machine', 'learning', 'job']]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MTrPBS73pVjp"
      },
      "source": [
        "# # saving lemma file\n",
        "with open(os.path.join(intermediate_path,'lemmawords.txt'),'w') as fp:\n",
        "  json.dump(data_lemmatized, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZJdxSX2fd8Z"
      },
      "source": [
        "# starting\n",
        "# of lda normal"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mzm1MgQpVjx",
        "outputId": "beb1a871-a01f-44c1-9156-c905bfc0c716"
      },
      "source": [
        "with open(os.path.join(intermediate_path,r'lemmawords.txt'), \"r\") as fp:\n",
        "  data_lemmatized = json.load(fp)\n",
        "len(data_lemmatized)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30201"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H217mAYFfd8d",
        "scrolled": true
      },
      "source": [
        "# 6\n",
        "\n",
        "# Creating  LDA model without NN\n",
        "\n",
        "id2word = corpora.Dictionary(data_lemmatized)\n",
        "texts = data_lemmatized\n",
        "corpuslda = [id2word.doc2bow(text) for text in texts]\n",
        "lda_model = gensim.models.LdaMulticore(corpus=corpuslda,\n",
        "                                           id2word=id2word,\n",
        "                                           num_topics=20, \n",
        "                                           random_state=100,\n",
        "                                           chunksize=100,\n",
        "                                           passes=10,\n",
        "                                           alpha=0.01,\n",
        "                                           eta=0.9)\n",
        "\n",
        "coherence_model_lda = CoherenceModel(model=lda_model, texts=data_lemmatized, dictionary=id2word, coherence='c_v')\n",
        "coherence_lda = coherence_model_lda.get_coherence()\n",
        "print('\\nCoherence Score: ', coherence_lda)\n",
        "pprint(lda_model.print_topics())\n",
        "doc_lda = lda_model[corpuslda]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SyDlQ_RXlDtR"
      },
      "source": [
        "# pyLDAvis.enable_notebook()\n",
        "# LDAvis_prepared = pyLDAvis.gensim.prepare(lda_model, corpuslda, id2word)\n",
        "# LDAvis_prepared\n",
        "pickle.dump(lda_model,open(os.path.join(intermediate_path,r\"topicmodelsimple.pb\"),'wb'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "okQbDQsyfd8k"
      },
      "source": [
        "# 7\n",
        "# LDA model with NN\n",
        "\n",
        "datapathnn=os.path.join(intermediate_path,r'cleantext_startend_data.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTgOmMXnfd8n",
        "outputId": "77ccb729-47f9-45dd-8564-1941bccfffc6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "datann = DocData(datapathnn, ns_rate=1, min_count=1)\n",
        "datann.count_cooccs(os.path.join(intermediate_path,r\"stt_lemmas.json\"))\n",
        "datann.load_cooccs(os.path.join(intermediate_path,r\"stt_lemmas.json\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading documents: 30200\n",
            "Vocabulary size: 31049\n",
            "Preparing data: 100%\n",
            "Counting word co-occurrences...\n",
            "Loading word co-occurrence data...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J8Bwxi-ufd8r",
        "scrolled": true,
        "outputId": "c337e5ed-060d-4a51-bd3a-772eba308bf8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        }
      },
      "source": [
        "# 8\n",
        "\n",
        "#  building model\n",
        "f=8\n",
        "lr=0.015\n",
        "n_topic=10\n",
        "n_epochs=2\n",
        "modelnn = Doc2Topic(datann, n_topics=n_topic, batch_size=1024*f, n_epochs=0, lr=lr, l1_doc=0.0000002, l1_word=0.000000015)\n",
        "logger = Logger(os.path.join(intermediate_path,r\"log_stt_full.csv\"), modelnn, custom_evaluator)\n",
        "modelnn.train(1, callbacks=[LambdaCallback(on_epoch_end=logger.record)])\n",
        "data2 = DocDatalstm(datapathnn, ns_rate=1, min_count=1)\n",
        "\n",
        "# data2.count_cooccs(\"stt_lemmas2.json\")\n",
        "# data2.load_cooccs(\"stt_lemmas2.json\")\n",
        "# model2 = Doc2Topiclstm(datalstm, n_topics=20, batch_size=1024*f, n_epochs=n_epochs, lr=lr, l1_doc=0.0000002, l1_word=0.000000015)\n",
        "# logger = Logger(\"log_stt_fulllda.csv\", modellstm, custom_evaluator)\n",
        "# model2.train(n_epochs, callbacks=[LambdaCallback(on_epoch_end=logger.record)])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " \n",
            "Train on 5169596 samples\n",
            "Train on 5169596 samples\n",
            "5169152/5169596 [============================>.] - ETA: 0s - loss: 0.5597 - fmeasure: 0.7781Doc L2/L1: 0.581\n",
            "Doc peakiness: 0.229\n",
            "\n",
            "Topic words\n",
            "0 (13.316): using, machinelearning, bigdata, intelligence, artificial, robotics, top, learn, machine, robots\n",
            "1 (12.756): fintech, artificialintelligence, machinelearning, use, robots, perl, robotic, every, python, google\n",
            "2 (13.212): using, business, data, machinelearning, artificial, bigdata, robotics, future, healthcare, next\n",
            "3 (13.181): via, iot, machinelearning, intelligence, artificial, robots, industry, use, perl, next\n",
            "4 (13.138): iot, fintech, artificialintelligence, artificial, robots, infographics, top, data, robotics, best\n",
            "5 (12.969): via, fintech, datascience, infographics, data, intelligence, industry, change, perl, future\n",
            "6 (12.556): iot, change, bigdata, robotics, industry, machine, best, future, news, realistic\n",
            "7 (13.211): datascience, business, artificialintelligence, data, bigdata, intelligence, robotics, every, new, use\n",
            "8 (12.984): artificialintelligence, using, intelligence, top, industry, check, machine, future, learn, read\n",
            "9 (12.924): datascience, business, news, change, machinelearning, artificial, artificialintelligence, learn, machine, check\n",
            "Mean semantic coherence: 13.025\n",
            "5169596/5169596 [==============================] - 23s 4us/sample - loss: 0.5597 - fmeasure: 0.7782\n",
            "Reading documents: 30200\n",
            "Vocabulary size: 31049\n",
            "Preparing data: 100%\n",
            "(51835,)\n",
            "(5183500,)\n",
            "(5183500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyUiYCN3fd81"
      },
      "source": [
        "topicwordnn=modelnn.get_topic_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LfnDLYzFfd84"
      },
      "source": [
        "# 9\n",
        "\n",
        "#creating topic list from above model\n",
        "topicdict={}\n",
        "toptopics=[]\n",
        "j=0\n",
        "for i in range(10):\n",
        "    for word,score in topicwordnn[i]:\n",
        "        if word in topicdict:\n",
        "            continue\n",
        "        else:\n",
        "            topicdict[word]=1\n",
        "            toptopics.append(word)\n",
        "    \n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f4mxYwypfd8-"
      },
      "source": [
        "# 10\n",
        "\n",
        "# Text summarization begin\n",
        "\n",
        "max_len_text=100   #max length of input comment\n",
        "max_len_summary=35 #max length of summary we want\n",
        "latent_dim=500\n",
        "\n",
        "# if you have already trained model than make want_train=0 \n",
        "want_train=1\n",
        "if want_train==1:\n",
        "    cleaned_summary=pd.read_csv(os.path.join(intermediate_path,r'cleansummary_startend_data.csv'))\n",
        "    cleaned_text=pd.read_csv(os.path.join(intermediate_path,r'cleantext_startend_data.csv'))\n",
        "    cleaned_summary=cleaned_summary['summary']\n",
        "    cleaned_text=cleaned_text['comment']\n",
        "    x_tr,x_val,y_tr,y_val=train_test_split(cleaned_text,cleaned_summary,test_size=.1,random_state=0,shuffle=True)\n",
        "    x_tokenizer = Tokenizer()\n",
        "    x_tokenizer.fit_on_texts(x_tr)\n",
        "    y_tokenizer = Tokenizer()\n",
        "    y_tokenizer.fit_on_texts(list(y_tr))\n",
        "    with open(os.path.join(intermediate_path,r'x_tokenizer.pickle'), 'wb') as handle:\n",
        "        pickle.dump(x_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "\n",
        "    with open(os.path.join(intermediate_path,r'y_tokenizer.pickle'), 'wb') as handle:\n",
        "        pickle.dump(y_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    with open(os.path.join(intermediate_path,r\"x_tokenizer.pickle\"), 'rb') as handle:\n",
        "        x_tokenizer = pickle.load(handle)\n",
        "\n",
        "    with open(os.path.join(intermediate_path,r\"y_tokenizer.pickle\"), 'rb') as handle:\n",
        "        y_tokenizer = pickle.load(handle)        \n",
        "    x_tr    =   x_tokenizer.texts_to_sequences(list(x_tr)) \n",
        "    x_val   =   x_tokenizer.texts_to_sequences(list(x_val))\n",
        "    x_tr    =   pad_sequences(x_tr,  maxlen=max_len_text, padding='post') \n",
        "    x_val   =   pad_sequences(x_val, maxlen=max_len_text, padding='post')\n",
        "    y_tr    =   y_tokenizer.texts_to_sequences(y_tr) \n",
        "    y_val   =   y_tokenizer.texts_to_sequences(y_val) \n",
        "    y_tr    =   pad_sequences(y_tr, maxlen=max_len_summary, padding='post')\n",
        "    y_val   =   pad_sequences(y_val, maxlen=max_len_summary, padding='post')\n",
        "else:\n",
        "    cleaned_text=pd.read_csv(os.path.join(intermediate_path,r'cleantext_startend_data.csv'))\n",
        "    cleaned_text=cleaned_text['comment']    \n",
        "    x_tr=cleaned_text\n",
        "    x_tokenizer = Tokenizer()\n",
        "    y_tokenizer = Tokenizer()\n",
        "    with open(os.path.join(intermediate_path,r'x_tokenizer.pickle'), 'rb') as handle:\n",
        "        x_tokenizer = pickle.load(handle)\n",
        "\n",
        "    with open(os.path.join(intermediate_path,r'y_tokenizer.pickle'), 'rb') as handle:\n",
        "        y_tokenizer = pickle.load(handle)\n",
        "    \n",
        "    x_val    =   x_tokenizer.texts_to_sequences(list(x_tr)) \n",
        "    x_val    =   pad_sequences(x_val,  maxlen=max_len_text, padding='post') \n",
        "        \n",
        "y_voc_size  =   len(y_tokenizer.word_index) +1\n",
        "x_voc_size  =   len(x_tokenizer.word_index) +1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GYsIjBOUfd9B",
        "outputId": "f98c7194-21ad-4f46-c6df-ce1c15da15f8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        }
      },
      "source": [
        "print(x_voc_size)\n",
        "print(y_voc_size)\n",
        "print(max_len_text)\n",
        "print(max_len_summary)\n",
        "print(latent_dim)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "41833\n",
            "27755\n",
            "100\n",
            "35\n",
            "500\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ae9ePsJ1fd9G"
      },
      "source": [
        "# 11\n",
        "\n",
        "#first we will define all the layers of the summarizer model than add them \n",
        "\n",
        "######encoder\n",
        "encoder_inputs=Input(shape=(max_len_text,))\n",
        "enc_emb=Embedding(x_voc_size,latent_dim,trainable=True)(encoder_inputs)\n",
        "encoder_lstm1=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "encoder_output1,state_h1,state_c1=encoder_lstm1(enc_emb)\n",
        "encoder_lstm2=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "encoder_outputs,state_h,state_c=encoder_lstm2(encoder_output1)\n",
        "decoder_inputs=Input(shape=(None,))\n",
        "dec_emb_layer=Embedding(y_voc_size,latent_dim,trainable=True)\n",
        "dec_emb=dec_emb_layer(decoder_inputs)\n",
        "decoder_lstm=LSTM(latent_dim,return_sequences=True,return_state=True)\n",
        "decoder_outputs,decoder_fwd_state,decoder_back_state=decoder_lstm(dec_emb,initial_state=[state_h,state_c])\n",
        "attn_layer=AttentionLayer(name='attention_layer')\n",
        "attn_out,attn_state=attn_layer([encoder_outputs,decoder_outputs])\n",
        "decoder_concat_input=Concatenate(axis=-1,name='concat_layer')([decoder_outputs,attn_out])\n",
        "decoder_dense=TimeDistributed(Dense(y_voc_size,activation='softmax'))\n",
        "decoder_outputs=decoder_dense(decoder_concat_input)\n",
        "modelsum=Model([encoder_inputs,decoder_inputs],decoder_outputs)\n",
        "\n",
        "# modelsum.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5JRR78TOfd9J"
      },
      "source": [
        "modelsum.compile(optimizer='rmsprop',loss='sparse_categorical_crossentropy')\n",
        "es=EarlyStopping(monitor='val_loss',mode='min',verbose=1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PEIO8vwfd9M"
      },
      "source": [
        "# 12\n",
        "\n",
        "\n",
        "# make_train =0 if using previously traind model\n",
        "want_train=1\n",
        "if want_train==1:\n",
        "    modelcheckpoint = ModelCheckpoint(os.path.join(intermediate_path,r\"lstm_summ.h5\"), save_best_only=True, verbose=1)\n",
        "    history=modelsum.fit([x_tr,y_tr[:,:-1]], y_tr.reshape(y_tr.shape[0],y_tr.shape[1], 1)[:,1:] ,epochs=15,callbacks=[es],batch_size=32, validation_data=([x_val,y_val[:,:-1]], y_val.reshape(y_val.shape[0],y_val.shape[1], 1)[:,1:]))\n",
        "else:\n",
        "    modelsum.load_weights(os.path.join(intermediate_path,r'kagglelstmsummarymodel.h5'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3sueX_Vfd9P"
      },
      "source": [
        "reverse_target_word_index=y_tokenizer.index_word\n",
        "reverse_source_word_index=x_tokenizer.index_word\n",
        "target_word_index=y_tokenizer.word_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "buRTv0d-fd9S"
      },
      "source": [
        "# 13\n",
        "\n",
        "\n",
        "#inference main part of language modelling\n",
        "\n",
        "#encoder\n",
        "encoder_model=Model(inputs=encoder_inputs,outputs=[encoder_outputs,state_h,state_c])\n",
        "decoder_state_input_h=Input(shape=(latent_dim,))\n",
        "decoder_state_input_c=Input(shape=(latent_dim,))\n",
        "decoder_hidden_state_input=Input(shape=(max_len_text,latent_dim))#this is the ouptut of encoder for each text\n",
        "dec_emb2=dec_emb_layer(decoder_inputs)\n",
        "decoder_outputs2,state_h2,state_c2=decoder_lstm(dec_emb2,initial_state=[decoder_state_input_h,decoder_state_input_c])\n",
        "attn_out_inf,attn_states_inf=attn_layer([decoder_hidden_state_input,decoder_outputs2])\n",
        "decoder_inf_concat=Concatenate(axis=-1,name='concat')([decoder_outputs2,attn_out_inf])\n",
        "decoder_outputs2=decoder_dense(decoder_inf_concat)\n",
        "decoder_model=Model([decoder_inputs]+[decoder_hidden_state_input,decoder_state_input_h,decoder_state_input_c],[decoder_outputs2]+[state_h2,state_c2])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SiHxxIpZfd9W",
        "outputId": "81131309-9ef6-4dfd-c282-3758536a8a39"
      },
      "source": [
        "# 14\n",
        "\n",
        "# summarizing the comment and saving them\n",
        "predictsumm=[]\n",
        "# over here 3(just for eg) represent number of comments you wants to summarize change it according to your wish \n",
        "for i in range(3):\n",
        "    seq2text(x_val[i],reverse_source_word_index)\n",
        "#     seq2summary(y_val[i],target_word_index,reverse_target_word_index)\n",
        "    prdtone=decode_sequence(x_val[i].reshape(1,max_len_text),encoder_model,decoder_model,target_word_index,reverse_target_word_index,max_len_summary)\n",
        "    predictsumm.append(prdtone)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From C:\\Users\\SANJEEV\\AppData\\Roaming\\Python\\Python37\\site-packages\\tensorflow\\python\\keras\\engine\\training_v1.py:2070: Model.state_updates (from tensorflow.python.keras.engine.training) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This property should not be used in TensorFlow 2.0, as updates are applied automatically.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQCZ8dkLfd9a"
      },
      "source": [
        "# ck=[\"Artificial intelligence (AI), sometimes called machine intelligence, is intelligence demonstrated by machines, unlike the natural intelligence displayed by humans and animals. Leading AI textbooks define the field as the study of intelligent agents: any device that perceives its environment and takes actions that maximize its chance of successfully achieving its goals.[1] Colloquially, the term artificial intelligence is often used to describe machines (or computers) that mimic cognitive functions that humans associate with the human mind, such as learning and problem solving\"]\n",
        "# ck.append(\"A robot is a machine—especially one programmable by a computer— capable of carrying out a complex series of actions automatically.[2] Robots can be guided by an external control device or the control may be embedded within. Robots may be constructed on the lines of human form, but most robots are machines designed to perform a task with no regard to their aesthetics.\")\n",
        "# ck.append(\"Computer Science is the study of computers and computational systems. Unlike electrical and computer engineers, computer scientists deal mostly with software and software systems; this includes their theory, design, development, and application\")\n",
        "\n",
        "\n",
        "# ck2=[]\n",
        "# for i in range(len(ck)):\n",
        "#     ck2.append(preprocessingsum(ck[i]))\n",
        "#     ck2\n",
        "# ck2   =   x_tokenizer.texts_to_sequences(list(ck2))\n",
        "# ck2    =   pad_sequences(ck2,  maxlen=max_len_text, padding='post')\n",
        "\n",
        "# predictsumm=[]\n",
        "# for i in range(len(ck)):\n",
        "#   print(i)\n",
        "#   prdtone=decode_sequence(ck2[i].reshape(1,max_len_text),encoder_model,decoder_model,target_word_index,reverse_target_word_index,max_len_summary)\n",
        "#   print(prdtone)\n",
        "#   predictsumm.append(prdtone)\n",
        "#   print(\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2nbIkFVfd9l"
      },
      "source": [
        "# optimization by firefly begin"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XczK8PFmfd9o"
      },
      "source": [
        "finaldict=[]\n",
        "for i in predictsumm:\n",
        "  finaldict.append(i.split())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nAflB5RCfd9q",
        "outputId": "b6af5de5-06dd-4f2e-ace6-e99f38c99172",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 15\n",
        "\n",
        "\n",
        "# loading and training fasttext model\n",
        "want_train=1\n",
        "if want_train==1:\n",
        "  model = fasttext.train_unsupervised(os.path.join(intermediate_path,r'lemmawords (2).txt'), model='skipgram',epoch=10)\n",
        "  model.save_model(os.path.join(intermediate_path,r\"fasttext10.bin\"))\n",
        "else:\n",
        "  model = fasttext.load_model(os.path.join(intermediate_path,r\"fasttext10.bin\"))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlrOIAAWfd9t"
      },
      "source": [
        "# creating word dictionary for fasttext\n",
        "fastdict=model.get_words()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u0WK11PEfd9x"
      },
      "source": [
        "prdsummary=copy.deepcopy(finaldict)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_NnfVSBrfd9z"
      },
      "source": [
        "# converting text summary to vector\n",
        "\n",
        "myflies=[]\n",
        "for i in range(len(prdsummary)):\n",
        "    for  k in prdsummary[i]:\n",
        "      myflies.append(model[k])    \n",
        "targetfly=[]\n",
        "for  k in toptopics:\n",
        "#   print(k)\n",
        "  targetfly.append(model[k])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "So_ZqJAIfd91"
      },
      "source": [
        "# without PCA\n",
        "\n",
        "\n",
        "# val = input(\"Enter the topic: \") \n",
        "# Best,topword,nearest=optimizeresult(flag=1,key=val,fastdict=fastdict,model=model,myflies=myflies,targetfly=targetfly,topicdict=topicdict,prdsummary=prdsummary)\n",
        "# print(topword)\n",
        "# print(nearest)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTxnl24afd97",
        "outputId": "70acd7c8-3a83-4b77-d442-b2737fd7b87f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "# 16\n",
        "\n",
        "# taking topic from the user as input and optimize the fireflies around this topic\n",
        "val = input(\"Enter the topic: \") \n",
        "bestplot,allplot,targetplot,topword,nearest=optimizeresultpca(flag=1,key=val,fastdict=fastdict,model=model,myflies=myflies,targetfly=targetfly,topicdict=topicdict,prdsummary=prdsummary)\n",
        "print(topword)\n",
        "print(nearest)\n",
        "\n",
        "# printing words nearest to this topc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Enter the topic: human\n",
            "[('\"realisticby\",', 0.7369333554928368), ('\"highly_realistic\",', 0.6923611462520299), ('\"realistic\"],', 0.6479103651149237), ('[\"realistic\",', 0.6468618225550896), ('\"realistically\",', 0.6375541529991952), ('\"realistic\",', 0.6334381839201929), ('[\"center_produced\",', 0.6289973187102745), ('\"thrilled_attend\",', 0.6269232707757895), ('\"realistic_motion\"],', 0.6265621564618401), ('[\"thrilled_attend\",', 0.6150404076722267)]\n",
            "[('fintech', 0.42551586), ('healthcare', 0.4201201), ('technology', 0.40327197), ('augmented', 0.3800638), ('techno', 0.3770635), ('robots', 0.37636667), ('technical', 0.35719898), ('artificialintelligence', 0.34623384), ('artificialintelligence', 0.34623384), ('artificialintelligence', 0.34623384)]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gPDV2yVspVmn"
      },
      "source": [
        "# 17\n",
        "\n",
        "with open(os.path.join(output_path,r\"topwords.txt\"), \"wb\") as fp:\n",
        "    pickle.dump(topword, fp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9NMV4nqrfd9_",
        "outputId": "08e59e7e-3a0b-4a73-ef32-b31c3d9359e8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 282
        }
      },
      "source": [
        "for i in allplot:\n",
        "  plt.scatter(i[0],i[1], marker='.')\n",
        "\n",
        "\n",
        "plt.scatter(targetplot[0],targetplot[1] ,marker='^')\n",
        "plt.scatter(bestplot[0],bestplot[1], marker='s')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x208c306ef08>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD4CAYAAAD8Zh1EAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAfTUlEQVR4nO3dbWxc53nm8f89MxwWkiuKkfwSyyOOCUuArSgrWjTNuFCSbRKv011Y+8HeOOu0zItsoEV2AecF8MJ1duHAQJMgyKaom42symEMbd3YaBthV23cOm5WWJeWSJGIIHutaNmhR5Yt2QpNxWaXw5lz74chFYoeSUPyzJyZM9cPIEjOHM7cRxQvPrznOc9j7o6IiDS/RNQFiIhIOBToIiIxoUAXEYkJBbqISEwo0EVEYiIV1ROvX7/es9lsVE8vItKURkZG3nL3KyvdF1mgZ7NZhoeHo3p6EZGmZGYTF7tPLRcRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEwo0EVEYkKBLiISEwp0aWojE5M89vwJRiYmoy5FJHKRXSkqsiz5Q5A7CNkdjASbuHfPEIViQDqVYN+ufrZ3dUZdoUhkFOjSPPKHYPBOKBUgmeafPvCnFIrtBA6zxYCh8bMKdGlparlI88gdLIe5l6BU4EPJl0inEiQN2lIJ+rvXRV2hSKQ0Qpfmkd0ByfT5EfqGbbez74ObGBo/S3/3Oo3OpeUp0KV5ZPpgYP/5HjqZPraDglxkjgJdmkumr/wmIu+hHrqISJXy+TwHDx4kn89HXUpFGqGLiFQhn88zODhIqVQimUwyMDBAJpOJuqwLaIQuIlKFXC5HqVTC3SmVSuRyuahLeg8FuohIFbLZLMlkEjMjmUzSiHsiq+UiIlKFTCbDwMAAuVyObDbbcO0WqCLQzWwv8G+AM+7+gQr3G/Bd4HeAaeCz7n4k7EJFRKKWyWQaMsjnVdNy+QFwxyXu/ySwae7tfuB7Ky9LRESW6rKB7u7/C/jlJQ7ZCfzQy4aAtWb2/rAKFBGR6oTxougGYOGkzJNzt4mISB2FEehW4TaveKDZ/WY2bGbDb775ZghPLRKdsTNj7Dm6h7EzY1GXIgKEM8vlJLDwVYLrgFOVDnT33cBugN7e3oqhL9IMxs6Mcd+z91EoFUgn0zx+++Nsu2pb1GVJiwtjhL4f+D0r6wem3P31EB5XpGENnx6mUCoQEDAbzDJ8ejjqkkSqmrb458BHgfVmdhL4z0AbgLv/N+AA5SmLJyhPW/xcrYoVaRS9V/eSTqaZDWZpS7TRe3Vv1CWJYO7RdD56e3t9eFijGmleY2fGGD49TO/VvWq3SN2Y2Yi7VxxB6EpRkWXadtU2Bbk0FK3lIiISEwp0qZ38ITj47fJ7Eak5tVykNvKHYPDO8/t/MrBfOw2J1JhG6FIbuYPlMPdS+X3uYNQVicSeAl1qI7ujPDK3ZPl9dkfUFcXazMQ5zj2fZ2biXNSlSITUcpHayPSV2yy5g+UwV7ulZmYmzvHWnqN4McBSCdbv2kp715qoy5IIKNCldjJ9CvI6mBmfwosBOHgxYGZ8SoHeotRyEWly7d0dWCoBBpZK0N7dEXVJEhGN0EWaXHvXGtbv2loemXd3aHTewhToIjHQ3rVGQS5quYiIxIUCXZrKyMQkjz1/gpGJyahLEWk4arlI0xiZmOTePUMUigHpVIJ9u/rZ3tUZdVkiDUMjdGkaQ+NnKRQDAofZYsDQ+NmoS5IVmJo6Qi73PaamjkRdSmxohC5No797HelUgtliQFsqQX/3uqhLkmWamjrCkdHfJQgKJBJpbu55ko6Om6Muq+kp0KVpbO/qZN+ufobGz9LfvU7tliY2OfkiQVAAAoJglsnJFxXoIVCgS1PZ3tWpII+Bzs5bSSTSBMEsiUQbnZ23Rl1SLCjQRepkeOpdXnj7HW5bewW9HatDfezp0VGmDx1mVd8trOrpCfWxa6Gj42Zu7nmSyckX6ey8VaPzkCjQRepgeOpd7ho7wWzgtCWMZ7bdEFqoT4+O8urnPo8XClg6zcYn9i4p1Gv5i+ZSOjpuVpCHTLNcROrghbffYTZwSsBs4Lzw9juhPfb0ocN4oQBBgM/OMn3ocNVfO/+L5hvjr3PX2AmGp94NrS6pPwW6SB3ctvYK2hJGEmhLGLetvSK0x17VdwuWTkMyibW1sarvlqq/tpa/aKT+1HIRqYPejtU8s+2GmrQ2VvX0sPGJvcvqoc//omGuFRTmLxqpP3P3SJ64t7fXh4eHI3luEfm1qHrosjxmNuLuvZXu0whdpErNNpOkWr0dqxXkMVFVoJvZHcB3gSSwx93/aNH9G4FBYO3cMQ+6+4GQaxWJzEpnkojUw2VfFDWzJPAY8EngJuDTZnbTosP+EPiRu/cA9wB/GnahIlFayUwSkXqpZpZLH3DC3cfdvQA8BexcdIwD86vrdwCnwitRJHormUkiUi/VtFw2APkFn58EFl+n+1+AZ83sPwCrgY9XeiAzux+4H2Djxo1LrVUkMiuZSSJSL9UEulW4bfHUmE8DP3D3b5vZh4AnzewD7h5c8EXuu4HdUJ7lspyCRaKyqqdHQS4NrZqWy0kgs+Dz63hvS+ULwI8A3P0fgd8A1odRoEhYZibOce75PDMT56IuRaQmqgn0w8AmM7vezNKUX/Tcv+iYV4GPAZjZjZQD/c0wC5XmdOr4y7z4Vz/i1PGXz98WxcYGMxPneGvPUc49m+OtPUcV6hJLl225uHvRzL4I/ITylMS97n7MzB4Bht19P/Bl4HEze4ByO+azHtUVS9IwTh1/mae//hClYpFkKsXdDz/K6qv/OZKNDWbGp/BiAA5eDJgZn6K9a83lv1CkiVQ1D31uTvmBRbd9bcHHLwG/FW5p0mzeGJ/iteOTbNjcyTXdHeSPHaVULOJBQKlYJH/sKFenz0aysUF7dweWSuDFAEslaO/uqPlzitSbrhSVULwxPsWPvzNKqRiQTCXY+UAPmS1bSaZS50fomS1bWd35z5FsbNDetYb1u7aWR+bdHRqdSywp0CUUrx2fpFQMcIdSKeC145Nsv+NG7n74UfLHjpLZspVrN98IENnGBu1daxTkEmsKdAnFhs2dJFMJSqWAZDLBhs3lbeKu3Xzj+SCfp40NRGpDgS6huKa7g50P9FzQQxeR+lKgS2iu6e5QkItESDsWiYjEhAJdRKQOhqfe5Y8nTtd031a1XEREamx+M+7Zua3+ntl2Q002FdEIXUSkxuq1GbcCXVpOPf70FVlofjPuJNR0M261XKSl1OtP32aXz+fJ5XJks1kymczlv0AuqbdjNc9su6Hmm3Er0KWlLPzTl7k/fRXoF8rn8wwODlIqlUgmkwwMDCjUQ1CPzbjVcpGWUq8/fZtZLpejVCrh7pRKJXK5XNQlSZU0QpeWUq8/fZtZNpslmUyeH6Fns9moS5IqWVTLlvf29vrw8HAkzy0il6YeeuMysxF37610n1ouEgt12wXpV2/Ad/8F/Op0bZ8nYplMhh07dijMm4xaLtL0pqaO1G8XpJ99E95+lf6//Fe8W35p9QKrU6sZuneoNs+9gEbQUokC/SIW774jjWty8sX67IL0qzdgbB94UDHMAd4t1n5uu2ahyMUo0CuotPuOQr1xdXbeWp9dkH72TfCgNo+9BJVmoSjQBRToFVXafUeB3rg6Om6u/S5I86PzUiH8x14izUKRi1GgV3Cx3XekcdV8F6QGGZ1D+QXLgYEB9dDlPRToFWj3HXmPVw40xOh8XiaTUZDLeyjQL0K778gFvvx/Lvh09b7+ii+Ark7pQiWJjgJdZBnqMTVRZKl0YZGISExUFehmdoeZvWJmJ8zswYsc8+/M7CUzO2Zm/z3cMkXkUt4Yn2Lkb3O8MT4VdSkSocu2XMwsCTwGfAI4CRw2s/3u/tKCYzYB/wn4LXefNLOralWwiFxI103IvGpG6H3ACXcfd/cC8BSwc9Ex9wGPufskgLufCbdMEbmYStdNSGuqJtA3APkFn5+cu22hzcBmM/vfZjZkZndUeiAzu9/Mhs1s+M0331xexSJygfnrJiyBrptocdXMcrEKty1eczcFbAI+ClwHHDSzD7j72xd8kftuYDeUl89dcrUi8h66bkLmVRPoJ4GFVzBcB5yqcMyQu88C/2Rmr1AO+MOhVCkil6TrJgSqa7kcBjaZ2fVmlgbuAfYvOuavgX8JYGbrKbdgxsMsVEQaQP4QHPx2+b00nMuO0N29aGZfBH4CJIG97n7MzB4Bht19/9x9t5vZS0AJ+Kq7n61l4RJ/Y2fGGD49TO/VvWy7alvU5cTCyMQkQ+Nn6e9ex/auJfba84dg8M7yEgjJNAzsh0xfbQqVZanqSlF3PwAcWHTb1xZ87MCX5t5EVmzszBj3PXsfhVKBdDLN47c/rlBfoZGJSe7dM0ShGJBOJdi3q39poZ47WA5zL5Xf5w4q0BtMS10pWrdtymTFhk8PUygVCAiYDWYZPq39Z1dqaPwshWJA4DBbDBgaX+If0dkd5ZG5JcvvsztqU6gsW8us5VLXbcpkxXqv7iWdTDMbzNKWaKP36op74sbGqeMvkz92lMyWrVy7+caaPEd/9zrSqQSzxYC2VIL+7nVLe4BMX7nNkjtYDnONzhtOywR63bYpk1Bsu2obj9/+eEv00E8df5mnv/4QpWKRZCrF3Q8/WpNQ397Vyb5d/cvvoUM5xBXkDSv2gT6/N2jnxi312aZMQrPtqm2xDvJ5+WNHKRWLeBBQKhbJHztas1H69q7O5QW5NIVYB/riNS4+8fvfI7HqWO22KRNZhsyWrSRTqfMj9MyWrVGXJE0q1oG+eI2LyVezbL/jw9V9cf6QeoVSF9duvpG7H3605j10ib9YB/qy9wbVfFups2s336ggX2RFc+ZbVKwDfdlrXGi+7WXl83ltUiw1s+I58y0q1oEOy1zjYn6+7fwIXfNtL5DP5xkcHKRUKpFMJhkYGFCoS6gqzZlvtECfHh1l+tBhVvXdwqqenqjLAVog0JdF820vKZfLUSqVcHdKpRK5XE6BLqFa8Zz5JZqaOsLk5ItVT5iYHh3l1c99Hi8UsHSajU/sbYhQV6BfjObbXlQ2myWZTJ4foWez2ahLkpgJZc58lZZz0eH0ocN4oQBBgM/OlkfqCnRpRplMhoGBAfXQpabqNWd+ORcdruq7BUun8dlZrK2NVX231LzOaijQZVkymUxrB7mmtcZGZ+etS77ocFVPDxuf2KseukjTq+O01pmJc8yMT9He3UF715rQH3+pveM46ui4mZt7nlzyv8Oqnp6GCfJ5CvQaqvUPo0SkTtNaZybO8daeo3gxwFIJ1u/aGur/Iy1Y92sdHTfH4txbavncepr/YTz3bI639hxlZuJc1CVJWOq0jOzM+BReDMDBiwEz41OhPn6l3rE0N43Qa6TSD6NG6TFRp2mt7d0dWCpxfoTeHvKeocvpHUtjU6DXSK1/GCVidZjW2t61hvW7ttasbbfc3nFU6rFmfLOz8u5x9dfb2+vDw/HehUY9dJFw1GvN+GZgZiPuXnHHF43Qa6i9a42CXCQE9VwzvpnpRVERaXjza8ZbIqE14y9BI3QRaXhaM746CnSRJRg7M9YS+5w2Iq0Zf3kKdJEqjZ0Z475n76NQKpBOpnn89scV6tJQquqhm9kdZvaKmZ0wswcvcdxdZuZmVvEVWJFmNnx6mEKpQEDAbDDL8Ol4z9KS5nPZQDezJPAY8EngJuDTZnZTheN+E/iPgC43k1jqvbqXdDJN0pK0JdrovVrjFmks1bRc+oAT7j4OYGZPATuBlxYd93Xgm8BXQq1QpEFsu2obj9/+uHroVdI2hfVXTaBvAPILPj8JXHCNsJn1ABl3/x9mdtFAN7P7gfsBNm7cuPRqRSK27aptCvIqaJvC9xqeepcX3n6H29ZeQW/H6po8RzU9dKtw2/nLS80sAXwH+PLlHsjdd7t7r7v3XnnlldVXKSJNpdI2hWEZnnqXP544zfDUu6E9Zq0NT73LXWMn+Mb469w1dqJmtVczQj8JLPzVeh1wasHnvwl8APgHMwO4BthvZne6u141akBvjE/x2vFJNmzuXPoG2iJVqNU2hfPBOBs4bQnjmW031Gy0G6YX3n6H2cApAQTOC2+/U5O6qwn0w8AmM7seeA24B/j383e6+xSwfv5zM/sH4CsK88b0xvgUP/7OKKViQDKVYOcDPQp1CV2ttimsVzCG7ba1V9CWMJj7RXTb2itq8jyXDXR3L5rZF4GfAElgr7sfM7NHgGF331+TyqQmXjs+SakY4A6lUsBrxycV6E1kenS04bY9u5habFNYr2AMW2/Hap7ZdkPNe+hVXVjk7geAA4tu+9pFjv3oysuSWtmwuZNkKkGpFJBMJtiwufab8DaCOCy9Oj06yquf+zxeKGDpNBuf2NvwoR62SsHYLN/b3o7VNf9rQleKtphrujvY+UBPS/XQL7X0ajO9njB96DBeKEAQ4LOz5ZF6iwU6XBiMWlb3Qgr0FnRNd0fDh1eYLrb0arO9nrCq7xYsncZnZ7G2Nlb13RJ1SZHTsroXUqCvgBZqag7zS6/Oj+Lml15tttcTVvX0sPGJvU3TQ6+Hi31vW5V2LFomLdQUrlpfdFGpz3p+hD73ekKjj9ClsmbpoYdFOxbVQKWFmhToy1OPucWVll5txdcT4kjL6v6aAn2Z5hdqmg1mtVDTCkU5t7jVXk+QeFOgL5MWagpPs84tFmk0CvQV0EJN4ajXRRcicadAl4ZQj4suROKuqh2LRESk8SnQRURiQoEuLWNm4hznns8zM3Eu6lJEakI99CrNTJxjZnyK9u4O2rvWRF2OLNHMxDne2nMULwZYKsH6XVv1fZTYUaBXQWHQ/GbGp/BiAA5eDMq/nPU9lJhRy6UKlcJAmkt7dweWSoCBpRK062IiiSEFehXqGQbTo6O89f3dTI+O1uw5WlF71xqu+HA7qbVnuOLD7bEenY+dGWPP0T2MnRmLuhSpM7VcqtDetYb1u7bWvIeuDQxqZ3p0lNe/eh9eKPD2n6dpi+m/baMvGqcVSmtLgV6l9q41NR/VaQOD2mmVf9tGXjSu0X/ZxIFaLg1kfgMDkkltYBCyVvm3nV80LmnJhls0rtIvGwmXRugNRBsY1E6r/Ns28qJxUaxQOjIxydD4Wfq717G9K/7752qDCxGpm3r20EcmJrl3zxCFYkA6lWDfrv5YhLo2uBCRhlDPFUqHxs9SKAYEDrPFgKHxs7EI9EtRD73Z5Q/BwW+X34vIef3d60inEiQN2lIJ+rvXRV1SzWmE3szyh2DwTigVIJmGgf2Q6Yu6qqaTz+fJ5XJks1kymUzU5UhItnd1sm9Xf0v10BXozSx3sBzmXiq/zx1UoC9RPp9ncHCQUqlEMplkYGBAoV4DUf3S3N7V2RJBPq+qlouZ3WFmr5jZCTN7sML9XzKzl8zs52b2nJl1hV+qvEd2R3lkbsny++yOqCtqOrlcjlKphLtTKpXI5XJRlxQ7+XyewR8M8tPnfsrgDwbJ5/NRlxRblw10M0sCjwGfBG4CPm1mNy06bBTodfcPAs8A3wy7UKkg01dus/z2Q2q3LFM2myWZTGJmJJNJstls1CXFzomxVygVizhOqVjkxNgrUZcUW9W0XPqAE+4+DmBmTwE7gZfmD3D35xccPwR8Jswi5RIyfQryFchkMgwMDKiHXkPvL60lQYLAAxIkeH9pbdQlxVY1gb4BWPg30kng1ksc/wXgbyrdYWb3A/cDbNy4scoSRWork8koyGvo+p7N/OuR05zyX3KtvY/rezZHXVJsVRPoVuG2ilcjmdlngF7gI5Xud/fdwG4oX1hUZY0i0sTau9awZdcObtAGMTVXTaCfBBYOX64DTi0+yMw+DjwEfMTdZ8IpT0TioB6L20l1s1wOA5vM7HozSwP3APsXHmBmPcD3gTvd/Uz4ZcpSTE0dIZf7HlNTR6IupamMTEzy2PMnGJmYjLoUkWW57Ajd3Ytm9kXgJ0AS2Ovux8zsEWDY3fcD3wKuAJ42M4BX3f3OGtYtFzE1dYQjo79LEBRIJNLc3PMkHR03R11Ww4vruh/SWqq6sMjdDwAHFt32tQUffzzkumSZJidfJAgKQEAQzDI5+aICvQqtuO6HxE9LreXSCltzdXbeSiKRBpIkEm10dl5qQpLMa8V1PyR+Wmb53FbaLWVq6giTky/S2XmrRudL0MxrZ586/jL5Y0fJbNnKtZtvjLocqSEtn0tjb80VloU/1NnNvx91OU2nWdf9OHX8ZZ7++kOUikWSqRR3P/yoQr1FtUygR7FbSj3ph7p15Y8dLV9aHwSUikXyx47qe9+iWibQG3lrrjDoh7p1ZbZsJZlKnf9lntmyNeqSJCItE+hQ391S6k0/1K3r2s03cvfDj6qHLq3zomgr0AtjIvGnF0VbxLWbb1SQi7SwlpqHLiISZwp0ucAb41OM/G2ON8anoi5FRJZILRc5743xKX78nVFKxYBkKsHOB3q4prsj6rJEpEoaoct5rx2fpFQMcIdSKeC141p1UKSZKNDlvA2bO0mmElgCkskEGzY331WTIq1MLRc575ruDnY+0MNrxyfZsLlT7RaRJqNAlwtc092hIBdpUmq5iIjEhAJdRCQmFOgiIjGhQJemNz06ylvf38306GjUpYhESi+KSlObHh3l1c99Hi8UsHSajU/sZVVPT9RliUSiaUfoIxOTPPb8CUYmdPFLK5s+dBgvFCAI8NlZpg8djrokkcg05Qh9ZGKSe/cMUSgGpFMJ9u3qb8qtw2TlVvXdgqXT+Ows1tbGqr5boi5JJDJNGehD42cpFAMCh9liwND4WQV6i1rV08PGJ/Yyfegwq/puUbtFWlpTBnp/9zrSqQSzxYC2VIL+7nVRlyQRWtXToyAXocpAN7M7gO8CSWCPu//RovvbgR8C24GzwKfcPRduqb+2vauTfbv6GRo/S3/3Oo3ORUSoItDNLAk8BnwCOAkcNrP97v7SgsO+AEy6+w1mdg/wDeBTtSh43vauTgW5iMgC1cxy6QNOuPu4uxeAp4Cdi47ZCQzOffwM8DEzs/DKFBGRy6km0DcA+QWfn5y7reIx7l4EpgA1tkVE6qiaQK800vZlHIOZ3W9mw2Y2/Oabb1ZTn4iIVKmaQD8JZBZ8fh1w6mLHmFkK6AB+ufiB3H23u/e6e++VV165vIpFRKSiagL9MLDJzK43szRwD7B/0TH7gYG5j+8Cfuru7xmhi4hI7Vx2lou7F83si8BPKE9b3Ovux8zsEWDY3fcDfwY8aWYnKI/M76ll0SIi8l5VzUN39wPAgUW3fW3Bx/8PuDvc0kREZCmadnEuERG5kAJdRCQmFOgiIjGhQBcRiQkFuohITCjQRURiQoEuIhITCnQRiV7+EBz8dvm9LFtT7lgkIjGSPwSDd0KpAMk0DOyHTF/UVTUljdBFJFq5g+Uw91L5fe5g1BU1LQW6iEQru6M8Mrdk+X12R9QVNS21XEQkWpm+cpsld7Ac5mq3LJsCXUSil+lTkIdALRcRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEyYu0fzxGZvAhORPHntrQfeirqIGtM5xoPOsfl0ufuVle6ILNDjzMyG3b036jpqSecYDzrHeFHLRUQkJhToIiIxoUCvjd1RF1AHOsd40DnGiHroIiIxoRG6iEhMKNBFRGJCgR4CM3ufmf2dmf1i7n3nRY7baGbPmtnLZvaSmWXrW+nyVXuOc8euMbPXzOxP6lnjSlVzjma2zcz+0cyOmdnPzexTUdS6VGZ2h5m9YmYnzOzBCve3m9lfzN3/YjP935xXxTl+ae7n7udm9pyZdUVRZy0p0MPxIPCcu28Cnpv7vJIfAt9y9xuBPuBMneoLQ7XnCPB14Gd1qSpc1ZzjNPB77r4FuAP4r2a2to41LpmZJYHHgE8CNwGfNrObFh32BWDS3W8AvgN8o75VrkyV5zgK9Lr7B4FngG/Wt8raU6CHYycwOPfxIPBvFx8w958r5e5/B+Du77j7dP1KXLHLniOAmW0HrgaerVNdYbrsObr7cXf/xdzHpyj/Uq541V4D6QNOuPu4uxeApyif60ILz/0Z4GNmZnWscaUue47u/vyCn7kh4Lo611hzCvRwXO3urwPMvb+qwjGbgbfN7C/NbNTMvjU3qmgWlz1HM0sA3wa+WufawlLN9/E8M+sD0sD/rUNtK7EByC/4/OTcbRWPcfciMAWsq0t14ajmHBf6AvA3Na0oAtqCrkpm9vfANRXueqjKh0gBO4Ae4FXgL4DPAn8WRn1hCOEc/wA44O75Rh3chXCO84/zfuBJYMDdgzBqq6FK34zF85WrOaaRVV2/mX0G6AU+UtOKIqBAr5K7f/xi95nZaTN7v7u/PveDXqk3fhIYdffxua/5a6CfBgr0EM7xQ8AOM/sD4AogbWbvuPul+u11FcI5YmZrgP8J/KG7D9Wo1DCdBDILPr8OOHWRY06aWQroAH5Zn/JCUc05YmYfp/zL+yPuPlOn2upGLZdw7AcG5j4eAH5c4ZjDQKeZzfdbfxt4qQ61heWy5+ju97r7RnfPAl8BfthIYV6Fy56jmaWBv6J8bk/XsbaVOAxsMrPr5+q/h/K5LrTw3O8CfurNddXhZc/RzHqA7wN3unszTUionrvrbYVvlHuNzwG/mHv/vrnbe4E9C477BPBz4CjwAyAdde1hn+OC4z8L/EnUdYd9jsBngFlgbMHbtqhrr+Lcfgc4Trnf/9DcbY9QDjeA3wCeBk4Ah4DuqGuuwTn+PXB6wfdtf9Q1h/2mS/9FRGJCLRcRkZhQoIuIxIQCXUQkJhToIiIxoUAXEYkJBbqISEwo0EVEYuL/Az0Fd/SYghv6AAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Cxrus9TpVmx",
        "outputId": "a393a995-fa92-4d92-b8a2-f68373ea7906"
      },
      "source": [
        "# 18\n",
        "\n",
        "\n",
        "# finding sentiment on the topic by the user\n",
        "ss=sent_polarity(topword,model,prdsummary,predictsumm)\n",
        "for k in sorted(ss):\n",
        "    print('{0}: {1}, '.format(k, ss[k]), end='')\n",
        "\n",
        "json.dump( ss, open(os.path.join(output_path,r\"polarity.npy\"), 'w' ) )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing\n",
            "1\n",
            " top influencers natural language processing used accelerate scientific discovery possible new materials via nlp artificialintelligence healthcare healthtech techno rapid pace innovation researchers samsung technology rapid pace innovation fintech\n",
            "compound: 0.8176, neg: 0.0, neu: 0.716, pos: 0.284, "
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdJkf16_pVm2"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}